{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document similarity comparison\n",
    "This is a short introduction to document similarity comparison in Python using the gensim library. It is based on Jonathan Mugan's outstanding [presentation](https://www.oreilly.com/learning/how-do-i-compare-document-similarity-using-python)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gensim\n",
    "gensim is a library that was written in order to generate lists of the most similar mathematical articles to a given article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbpresent": {
     "id": "c5728623-11b4-43dd-93b0-9b00335e1bda"
    }
   },
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a collections of \"documents\"\n",
    "For the purpose of simplicity the documents in this introduction are only sentences. But the principles are the same for more extensive documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "nbpresent": {
     "id": "df908d7f-4b59-47a5-8866-0a13ee385f2f"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 5\n"
     ]
    }
   ],
   "source": [
    "raw_documents = [\"I'm taking the show on the road.\",\n",
    "                 \"My socks are a force multiplier.\",\n",
    "                 \"I am the barber who cuts everyone's hair who doesn't cut their own.\",\n",
    "                 \"Legend has it that the mind is a mad monkey.\",\n",
    "                 \"I make my own fun.\"]\n",
    "print(\"Number of documents:\", len(raw_documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "2dd0f1a3-3832-4599-a9a5-3f366ae59eca"
    }
   },
   "source": [
    "## Tokenizing the documents\n",
    "The first step in this technique is to tokenize the documents,\n",
    "i.e. to break down the documents into the elements (tokens) that\n",
    "make them up.\n",
    "\n",
    "I use NLTK which is a widely used Python package for natural\n",
    "language processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "nbpresent": {
     "id": "7e6ad2c0-8465-4a01-92f1-4b93a186c3c9"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i', \"'m\", 'taking', 'the', 'show', 'on', 'the', 'road', '.'], ['my', 'socks', 'are', 'a', 'force', 'multiplier', '.'], ['i', 'am', 'the', 'barber', 'who', 'cuts', 'everyone', \"'s\", 'hair', 'who', 'does', \"n't\", 'cut', 'their', 'own', '.'], ['legend', 'has', 'it', 'that', 'the', 'mind', 'is', 'a', 'mad', 'monkey', '.'], ['i', 'make', 'my', 'own', 'fun', '.']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tokenized_docs = [[w.lower() for w in word_tokenize(text)]\n",
    "                     for text in raw_documents]\n",
    "print(tokenized_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "83626af7-e851-49c5-a0bf-4c4b706a1a9b"
    }
   },
   "source": [
    "## Creating a dictionary\n",
    "The dictionary maps the words in raw_documents to an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "nbpresent": {
     "id": "8545d2b1-8c65-4847-9963-92a24d646616"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 'm\n",
      "1 .\n",
      "2 i\n",
      "3 on\n",
      "4 road\n",
      "5 show\n",
      "6 taking\n",
      "7 the\n",
      "8 a\n",
      "9 are\n",
      "10 force\n",
      "11 multiplier\n",
      "12 my\n",
      "13 socks\n",
      "14 's\n",
      "15 am\n",
      "16 barber\n",
      "17 cut\n",
      "18 cuts\n",
      "19 does\n",
      "20 everyone\n",
      "21 hair\n",
      "22 n't\n",
      "23 own\n",
      "24 their\n",
      "25 who\n",
      "26 has\n",
      "27 is\n",
      "28 it\n",
      "29 legend\n",
      "30 mad\n",
      "31 mind\n",
      "32 monkey\n",
      "33 that\n",
      "34 fun\n",
      "35 make\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(tokenized_docs)\n",
    "for i in range(len(dictionary)):\n",
    "    print(i, dictionary[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "ebf3be16-8645-4528-a132-4fef93ba0612"
    }
   },
   "source": [
    "## Creating a corpus\n",
    "We're interested in counting how many times a word occurs in a document. To do this we create a corpus, which is a list of bag of words. A bag of words is a representation of a document that lists the number of times a word occurs in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "nbpresent": {
     "id": "c583d4d5-d62e-4658-9767-f8ba12dc9205"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 2)], [(1, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1)], [(1, 1), (2, 1), (7, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 2)], [(1, 1), (7, 1), (8, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1)], [(1, 1), (2, 1), (12, 1), (23, 1), (34, 1), (35, 1)]]\n"
     ]
    }
   ],
   "source": [
    "corpus = [dictionary.doc2bow(tokenized_doc) for tokenized_doc in tokenized_docs]\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "12875a47-621d-4e2a-8236-9996169883ca"
    }
   },
   "source": [
    "## Creating a tf-idf model\n",
    "tf-idf is short for 'term frequency-inverse document frequency'. This is used for calculating how important a word is to a document in a collection/corpus. Simply put, if a word occurs many times in a document but only a few times across the corpus, the word get a high tf-idf score.\n",
    "\n",
    "The tfâ€“idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general.(https://bit.ly/2bm6Qgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "nbpresent": {
     "id": "3e906efc-3720-411f-b463-c960e197cd1a"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfModel(num_docs=5, num_nnz=47)\n",
      "47\n"
     ]
    }
   ],
   "source": [
    "tf_idf = gensim.models.TfidfModel(corpus)\n",
    "print(tf_idf)\n",
    "s = 0\n",
    "for i in corpus:\n",
    "    s += len(i)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "bcea3301-b3c4-4079-a913-d1545f9c3aae"
    }
   },
   "source": [
    "## Creating a similarity measure\n",
    "This is going to create a similarity index for the five documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "nbpresent": {
     "id": "0de0f36f-a584-4f9d-93e4-bfd0dc18af65"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity index with 5 documents in 0 shards (stored under )\n",
      "<class 'gensim.similarities.docsim.Similarity'>\n"
     ]
    }
   ],
   "source": [
    "sims = gensim.similarities.Similarity('',tf_idf[corpus],\n",
    "                                      num_features=len(dictionary))\n",
    "print(sims)\n",
    "print(type(sims))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "767a2451-d2e5-4272-9a3a-8d3a33708b71"
    }
   },
   "source": [
    "## Creating a query document\n",
    "I'm gonna create the document that I want to compare to the five documents in raw_documents and the convert it to tf-idf. I'm also going to make a series of prints to show how the query document transforms from a text string to a list of tokens, a bag of words and tf-idf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "nbpresent": {
     "id": "fe94f9ea-a7ff-48ed-942c-ca629094dc83"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['socks', 'are', 'a', 'force', 'for', 'good', '.']\n",
      "[(1, 1), (8, 1), (9, 1), (10, 1), (13, 1)]\n",
      "[(8, 0.31226270667960454), (9, 0.5484803253891997), (10, 0.5484803253891997), (13, 0.5484803253891997)]\n"
     ]
    }
   ],
   "source": [
    "query_doc = [w.lower() for w in word_tokenize(\"Socks are a force for good.\")]\n",
    "print(query_doc)\n",
    "query_doc_bow = dictionary.doc2bow(query_doc)\n",
    "print(query_doc_bow)\n",
    "query_doc_tf_idf = tf_idf[query_doc_bow]\n",
    "print(query_doc_tf_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "76d5a7b9-625a-41fb-be60-c458c22cf800"
    }
   },
   "source": [
    "## Comparing the query document to raw_documents\n",
    "Finally I'm going to do a comparison that shows how similar the query document is to the five documents in raw_documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "nbpresent": {
     "id": "934f4c9c-ffcc-4ce0-b094-47734807f0b7"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.84565616, 0.        , 0.06124881, 0.        ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims[query_doc_tf_idf]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "61c5a0e2-5345-4953-bc8b-298742a05b04"
    }
   },
   "source": [
    "## Conclusion\n",
    "It shows that the query document \"Socks are a force for good.\" is most similar to the second document (\"My socks are a force multiplier.\" of raw_documents which makes good sense."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
